{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras.callbacks as cb\n",
    "import keras.utils.np_utils as np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers.core import Activation\n",
    "from keras import applications # For easy loading the VGG_16 Model\n",
    "from skimage import color\n",
    "import cv2\n",
    "# Image loading and other helper functions\n",
    "import dwdii_bc_model_helper as bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image_Augmentation\n",
    "The following function takes the 8bit grayscale images that we are using and performs a series of affine transformations to the images. There are vertical and horizontal flips along with rotations of 90, 270, 15, 30, and 45 degrees. Also included is a function for generating the rotations. Image augmentation needs to be performed before runnin the VGG_Prep function.  \n",
    "\n",
    "When calling the Image_Augmentation function setting the various flags to True will cause the transformation to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for rotating the image files.\n",
    "def Image_Rotate(img, angle):\n",
    "    \"\"\"\n",
    "    Rotates a given image the requested angle. Returns the rotated image.\n",
    "    \"\"\"\n",
    "    rows,cols = img.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2), angle, 1)\n",
    "    return(cv2.warpAffine(img,M,(cols,rows)))\n",
    "\n",
    "# Function for augmenting the images\n",
    "def Image_Augment(X, Y, vflip=False, hflip=False, major_rotate=False, minor_rotate=False):\n",
    "    \"\"\"\n",
    "    :param  X np.array of images\n",
    "            Y np.array of labels\n",
    "            vflip, hflip, major_rotate, minor_rotate set to True to perform the augmentations\n",
    "    :return The set of augmented iages and their corresponding labels\n",
    "    \n",
    "    \"\"\"\n",
    "    if len(X) != len(Y):\n",
    "        print('Data and Label arrays not of the same length.')\n",
    "    \n",
    "    n = vflip + hflip + 2*major_rotate + 6*minor_rotate\n",
    "    augmented = np.zeros([len(X) + n*len(X), X.shape[1], X.shape[2]])\n",
    "    label = np.zeros([len(Y) + n*len(Y), 1])\n",
    "    count = 0\n",
    "    for i in range(0, len(X)):\n",
    "        augmented[count] = X[i]\n",
    "        label[count] = Y[i]\n",
    "        count += 1\n",
    "        if vflip:\n",
    "            aug = cv2.flip(X[i], 0)\n",
    "            augmented[count] = aug\n",
    "            label[count] = Y[i]\n",
    "            count += 1\n",
    "        if hflip:\n",
    "            aug = cv2.flip(X[i], 1)\n",
    "            augmented[count] = aug\n",
    "            label[count] = Y[i]\n",
    "            count +=1 \n",
    "        if major_rotate:\n",
    "            angles = [90, 270]\n",
    "            for angle in angles:\n",
    "                aug = Image_Rotate(X[i], angle)\n",
    "                augmented[count] = aug\n",
    "                label[count] = Y[i]\n",
    "                count += 1\n",
    "        if minor_rotate:\n",
    "            angles = [-45,-30,-15,15,30,45]\n",
    "            for angle in angles:\n",
    "                aug = Image_Rotate(X[i], angle)\n",
    "                augmented[count] = aug\n",
    "                label[count] = Y[i]\n",
    "                count += 1\n",
    "                \n",
    "    return(augmented, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_Prep\n",
    "The following function takes the 8bit grayscale images that we are using and converts them to 8bit rgb while at the same time changing the pixles to a scale of 0 to 255. These image parameters are required by the VGG_16 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG_Prep(img_data):\n",
    "    \"\"\"\n",
    "    :param img_data: training or test images of shape [#images, height, width]\n",
    "    :return: the array transformed to the correct shape for the VGG network\n",
    "                shape = [#images, height, width, 3] transforms to rgb and reshapes\n",
    "    \"\"\"\n",
    "    images = np.zeros([len(img_data), img_data.shape[1], img_data.shape[2], 3])\n",
    "    for i in range(0, len(img_data)):\n",
    "        im = 255 - (img_data[i] * 255)  # Orginal imagnet images were not rescaled\n",
    "        im = color.gray2rgb(im)\n",
    "        images[i] = im\n",
    "    return(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_16 Bottleneck\n",
    "The following function leverages Daniel's image loader function and performs the following:\n",
    "1. Loads in the images using the train, test, and validation csv files.\n",
    "2. Prepares the images using the VGG_Prep function\n",
    "3. Loads the VGG_16 model with the cassification layers removed.\n",
    "4. Runs each of the images for the training, test, and validation sets (if included) through the model.\n",
    "5. Saves out .npy files containing the bottleneck features from the VGG_16 model predictions and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg16_bottleneck(trainPath, testPath, imagePath, modelPath, size, balance = True, verbose = True, \n",
    "                     verboseFreq = 50, valPath = 'None', transform = False):\n",
    "    # Loading data\n",
    "    metaTr, metaTr2, mCountsTr = bc.load_training_metadata(trainPath, balance, verbose)\n",
    "    lenTrain = len(metaTr)\n",
    "    X_train, Y_train = bc.load_data(trainPath, imagePath, maxData = lenTrain, verboseFreq = verboseFreq, imgResize=size)\n",
    "    \n",
    "    metaTest, meataT2, mCountsT = bc.load_training_metadata(testPath, balance, verbose)\n",
    "    lenTest = len(metaTest)\n",
    "    X_test, Y_test = bc.load_data(testPath, imagePath, maxData = lenTrain, verboseFreq = verboseFreq, imgResize=size)\n",
    "    \n",
    "    if transform:\n",
    "        print('Transforming the Training Data')\n",
    "        X_train, Y_train = Image_Augment(X=X_train, Y=Y_train, hflip=True, vflip=True, minor_rotate=True, major_rotate=True)\n",
    "    \n",
    "    print('Preparing the Training Data for the VGG_16 Model.')\n",
    "    X_train = VGG_Prep(X_train)\n",
    "    print('Preparing the Test Data for the VGG_16 Model')\n",
    "    X_test = VGG_Prep(X_test)\n",
    "        \n",
    "    print('Loading the VGG_16 Model')\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "        \n",
    "    # Generating the bottleneck features for the training data\n",
    "    print('Evaluating the VGG_16 Model on the Training Data')\n",
    "    bottleneck_features_train = model.predict(X_train)\n",
    "    \n",
    "    # Saving the bottleneck features for the training data\n",
    "    featuresTrain = os.path.join(modelPath, 'bottleneck_features_train.npy')\n",
    "    labelsTrain = os.path.join(modelPath, 'labels_train.npy')\n",
    "    print('Saving the Training Data Bottleneck Features.')\n",
    "    np.save(open(featuresTrain, 'wb'), bottleneck_features_train)\n",
    "    np.save(open(labelsTrain, 'wb'), Y_train)\n",
    "\n",
    "    # Generating the bottleneck features for the test data\n",
    "    print('Evaluating the VGG_16 Model on the Test Data')\n",
    "    bottleneck_features_test = model.predict(X_test)\n",
    "    \n",
    "    # Saving the bottleneck features for the test data\n",
    "    featuresTest = os.path.join(modelPath, 'bottleneck_features_test.npy')\n",
    "    labelsTest = os.path.join(modelPath, 'labels_test.npy')\n",
    "    print('Saving the Test Data Bottleneck Feaures.')\n",
    "    np.save(open(featuresTest, 'wb'), bottleneck_features_test)\n",
    "    np.save(open(labelsTest, 'wb'), Y_test)\n",
    "    \n",
    "    if valPath != 'None':\n",
    "        metaVal, metaV2, mCountsV = bc.load_training_metadata(valPath, verbose = verbose, balanceViaRemoval = False)\n",
    "        lenVal = len(metaVal)\n",
    "        X_val, Y_val = bc.load_data(valPath, imagePath, maxData = lenVal, verboseFreq = verboseFreq, imgResize=size)\n",
    "        X_val = VGG_Prep(X_val)\n",
    "        \n",
    "        # Generating the bottleneck features for the test data\n",
    "        print('Evaluating the VGG_16 Model on the Validataion Data')\n",
    "        bottleneck_features_val = model.predict(X_val)\n",
    "    \n",
    "        # Saving the bottleneck features for the test data\n",
    "        featuresVal = os.path.join(modelPath, 'bottleneck_features_validation.npy')\n",
    "        labelsVal = os.path.join(modelPath, 'labels_validation.npy')\n",
    "        print('Saving the Validation Data Bottleneck Features.')\n",
    "        np.save(open(featuresVal, 'wb'), bottleneck_features_val)\n",
    "        np.save(open(labelsVal, 'wb'), Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model on the Train, Test, and Validation Data\n",
    "1) The first test is on the rescaled and squared off images maintaining aspect ratio without the artifacts removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global variables for loading the data\n",
    "imagePath = '../images/ddsm/png/'\n",
    "trainDataPath = '../images/ddsm/ddsm_train.csv'\n",
    "testDataPath = '../images/ddsm/ddsm_test.csv'\n",
    "valDataPath = '../images/ddsm/ddsm_val.csv'\n",
    "imgResize = (224, 224) # can go up to (224, 224)\n",
    "modelPath = '../model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Balance\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 2685\n",
      "balanaceViaRemoval.avgE: 1318\n",
      "balanaceViaRemoval.theshold: 1318.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 862\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 2685\n",
      "balanaceViaRemoval.avgE: 1318\n",
      "balanaceViaRemoval.theshold: 1318.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 862\n",
      "0.0000: C_0418_1.LEFT_CC.LJPEG.png\n",
      "0.0235: C_0325_1.LEFT_MLO.LJPEG.png\n",
      "0.0469: B_3401_1.LEFT_CC.LJPEG.png\n",
      "0.0704: C_0288_1.LEFT_MLO.LJPEG.png\n",
      "0.0938: A_1097_1.LEFT_CC.LJPEG.png\n",
      "0.1173: B_3362_1.RIGHT_CC.LJPEG.png\n",
      "0.1407: B_3037_1.LEFT_MLO.LJPEG.png\n",
      "0.1642: B_3399_1.RIGHT_CC.LJPEG.png\n",
      "0.1876: B_3031_1.LEFT_CC.LJPEG.png\n",
      "0.2111: A_0263_1.LEFT_MLO.LJPEG.png\n",
      "0.2345: C_0400_1.RIGHT_MLO.LJPEG.png\n",
      "0.2580: B_3603_1.RIGHT_MLO.LJPEG.png\n",
      "0.2814: B_3154_1.LEFT_CC.LJPEG.png\n",
      "0.3049: C_0072_1.LEFT_CC.LJPEG.png\n",
      "0.3283: C_0136_1.LEFT_MLO.LJPEG.png\n",
      "0.3518: B_3073_1.LEFT_MLO.LJPEG.png\n",
      "0.3752: B_3062_1.LEFT_CC.LJPEG.png\n",
      "0.3987: B_3045_1.LEFT_CC.LJPEG.png\n",
      "0.4221: A_1064_1.LEFT_MLO.LJPEG.png\n",
      "0.4456: C_0246_1.RIGHT_MLO.LJPEG.png\n",
      "0.4690: A_1073_1.LEFT_CC.LJPEG.png\n",
      "0.4925: B_3510_1.RIGHT_MLO.LJPEG.png\n",
      "0.5159: A_0295_1.RIGHT_MLO.LJPEG.png\n",
      "0.5394: A_1057_1.RIGHT_CC.LJPEG.png\n",
      "0.5629: A_1077_1.LEFT_CC.LJPEG.png\n",
      "0.5863: C_0182_1.RIGHT_CC.LJPEG.png\n",
      "0.6098: C_0379_1.LEFT_MLO.LJPEG.png\n",
      "0.6332: C_0409_1.RIGHT_MLO.LJPEG.png\n",
      "0.6567: B_3099_1.RIGHT_MLO.LJPEG.png\n",
      "0.6801: C_0278_1.RIGHT_MLO.LJPEG.png\n",
      "0.7036: C_0032_1.LEFT_CC.LJPEG.png\n",
      "0.7270: B_3600_1.LEFT_CC.LJPEG.png\n",
      "0.7505: C_0080_1.LEFT_MLO.LJPEG.png\n",
      "0.7739: B_3058_1.RIGHT_CC.LJPEG.png\n",
      "0.7974: C_0009_1.RIGHT_CC.LJPEG.png\n",
      "0.8208: C_0246_1.LEFT_CC.LJPEG.png\n",
      "0.8443: B_3123_1.RIGHT_CC.LJPEG.png\n",
      "0.8677: B_3433_1.LEFT_MLO.LJPEG.png\n",
      "0.8912: B_3118_1.LEFT_CC.LJPEG.png\n",
      "0.9146: A_0322_1.RIGHT_CC.LJPEG.png\n",
      "0.9381: B_3093_1.RIGHT_MLO.LJPEG.png\n",
      "0.9615: C_0385_1.LEFT_MLO.LJPEG.png\n",
      "0.9850: A_0608_1.LEFT_MLO.LJPEG.png\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 658\n",
      "balanaceViaRemoval.avgE: 326\n",
      "balanaceViaRemoval.theshold: 326.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 215\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 658\n",
      "balanaceViaRemoval.avgE: 326\n",
      "balanaceViaRemoval.theshold: 326.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 215\n",
      "0.0000: B_3380_1.RIGHT_MLO.LJPEG.png\n",
      "0.0235: B_3013_1.LEFT_MLO.LJPEG.png\n",
      "0.0469: C_0361_1.LEFT_MLO.LJPEG.png\n",
      "0.0704: B_3478_1.LEFT_MLO.LJPEG.png\n",
      "0.0938: C_0286_1.RIGHT_MLO.LJPEG.png\n",
      "0.1173: B_3003_1.RIGHT_MLO.LJPEG.png\n",
      "0.1407: C_0166_1.RIGHT_CC.LJPEG.png\n",
      "0.1642: B_3357_1.LEFT_MLO.LJPEG.png\n",
      "0.1876: B_3084_1.RIGHT_MLO.LJPEG.png\n",
      "0.2111: B_3394_1.LEFT_CC.LJPEG.png\n",
      "0.2345: A_1092_1.LEFT_MLO.LJPEG.png\n",
      "Preparing the images for the VGG_16 Model\n",
      "Preparing the images for the VGG_16 Model\n",
      "Loading the VGG_16 Model\n",
      "Evaluating the VGG_16 Model on the Training Data\n",
      "Evaluating the VGG_16 Model on the Test Data\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 18\n",
      "malignant 34\n",
      "normal 142\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 18\n",
      "malignant 34\n",
      "normal 142\n",
      "balanaceViaRemoval.avgE: 64\n",
      "balanaceViaRemoval.theshold: 64.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 18\n",
      "malignant 34\n",
      "normal 38\n",
      "0.0000: C_0062_1.LEFT_CC.LJPEG.png\n",
      "0.2577: B_3402_1.LEFT_MLO.LJPEG.png\n",
      "Preparing the images for the VGG_16 Model\n",
      "Evaluating the VGG_16 Model on the Validataion Data\n"
     ]
    }
   ],
   "source": [
    "vgg16_bottleneck(trainDataPath, testDataPath, imagePath, modelPath, imgResize, \n",
    "                 balance = True, verbose = True, verboseFreq = 50, valPath = valDataPath, \n",
    "                 transform = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Top Model\n",
    "This function takes the bottleneck features from the bottleneck function and applies a shallow CNN to these features to classify the images. The function needs to be pointed at the locations of the training and test features along with the training and test labels. You can use the epoch and batch size variables to control the number of images to show to the model and the number of training epochs. The model save variabler alows for saving of the final model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_model(train_feats, train_lab, test_feats, test_lab, model_path, model_save, epoch = 50, batch = 64):\n",
    "    train_bottleneck = os.path.join(model_path, train_feats)\n",
    "    train_labels = os.path.join(model_path, train_lab)\n",
    "    test_bottleneck = os.path.join(model_path, test_feats)\n",
    "    test_labels = os.path.join(model_path, test_lab)\n",
    "    \n",
    "    history = LossHistory()\n",
    "    \n",
    "    X_train = np.load(train_bottleneck)\n",
    "    Y_train = np.load(train_labels)\n",
    "    Y_train = np_utils.to_categorical(Y_train, nb_classes=3)\n",
    "    \n",
    "    X_test = np.load(test_bottleneck)\n",
    "    Y_test = np.load(test_labels)\n",
    "    Y_test = np_utils.to_categorical(Y_test, nb_classes=3)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    # try Adadelta and Adam\n",
    "    model.compile(optimizer='adadelta',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              nb_epoch=epoch,\n",
    "              batch_size=batch,\n",
    "              callbacks=[history],\n",
    "              validation_data=(X_test, Y_test),\n",
    "              verbose=2)\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, batch_size=16, verbose=0)\n",
    "\n",
    "    print \"Network's test score [loss, accuracy]: {0}\".format(score)\n",
    "    \n",
    "    model.save_weights(model_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Top Model\n",
    "The following runs the top model classifier on the bottleneck features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locations for the bottleneck and labels files that we need\n",
    "modelPath = '../model/'\n",
    "train_bottleneck = 'bottleneck_features_train.npy'\n",
    "train_labels = 'labels_train.npy'\n",
    "test_bottleneck = 'bottleneck_features_test.npy'\n",
    "test_labels = 'labels_test.npy'\n",
    "validation_bottleneck = 'bottleneck_features_valdation.npy'\n",
    "validation_label = 'labels_validation.npy'\n",
    "top_model_weights_path = 'top_weights02.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2132 samples, validate on 536 samples\n",
      "Epoch 1/50\n",
      "0s - loss: 6.5851 - acc: 0.4508 - val_loss: 5.6644 - val_acc: 0.4571\n",
      "Epoch 2/50\n",
      "0s - loss: 3.4335 - acc: 0.5563 - val_loss: 2.2755 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "0s - loss: 1.0971 - acc: 0.6393 - val_loss: 1.2428 - val_acc: 0.5243\n",
      "Epoch 4/50\n",
      "0s - loss: 0.6560 - acc: 0.7317 - val_loss: 1.3331 - val_acc: 0.4963\n",
      "Epoch 5/50\n",
      "0s - loss: 0.5417 - acc: 0.7767 - val_loss: 1.3829 - val_acc: 0.5019\n",
      "Epoch 6/50\n",
      "0s - loss: 0.4130 - acc: 0.8354 - val_loss: 1.4304 - val_acc: 0.5392\n",
      "Epoch 7/50\n",
      "0s - loss: 0.3163 - acc: 0.8752 - val_loss: 1.5420 - val_acc: 0.5187\n",
      "Epoch 8/50\n",
      "0s - loss: 0.2728 - acc: 0.8963 - val_loss: 1.5069 - val_acc: 0.5168\n",
      "Epoch 9/50\n",
      "0s - loss: 0.2050 - acc: 0.9282 - val_loss: 1.6816 - val_acc: 0.5243\n",
      "Epoch 10/50\n",
      "0s - loss: 0.1788 - acc: 0.9273 - val_loss: 1.6940 - val_acc: 0.5485\n",
      "Epoch 11/50\n",
      "0s - loss: 0.1626 - acc: 0.9465 - val_loss: 1.7204 - val_acc: 0.5299\n",
      "Epoch 12/50\n",
      "0s - loss: 0.1236 - acc: 0.9578 - val_loss: 1.7611 - val_acc: 0.5243\n",
      "Epoch 13/50\n",
      "0s - loss: 0.1066 - acc: 0.9662 - val_loss: 1.8645 - val_acc: 0.5112\n",
      "Epoch 14/50\n",
      "0s - loss: 0.0929 - acc: 0.9662 - val_loss: 1.8626 - val_acc: 0.5280\n",
      "Epoch 15/50\n",
      "0s - loss: 0.0764 - acc: 0.9784 - val_loss: 1.8900 - val_acc: 0.5373\n",
      "Epoch 16/50\n",
      "1s - loss: 0.0669 - acc: 0.9761 - val_loss: 1.8557 - val_acc: 0.5317\n",
      "Epoch 17/50\n",
      "0s - loss: 0.0542 - acc: 0.9878 - val_loss: 1.9861 - val_acc: 0.5448\n",
      "Epoch 18/50\n",
      "0s - loss: 0.0424 - acc: 0.9892 - val_loss: 2.0198 - val_acc: 0.5578\n",
      "Epoch 19/50\n",
      "0s - loss: 0.0433 - acc: 0.9883 - val_loss: 2.0364 - val_acc: 0.5448\n",
      "Epoch 20/50\n",
      "0s - loss: 0.0405 - acc: 0.9892 - val_loss: 2.5172 - val_acc: 0.5131\n",
      "Epoch 21/50\n",
      "0s - loss: 0.0325 - acc: 0.9911 - val_loss: 2.1068 - val_acc: 0.5578\n",
      "Epoch 22/50\n",
      "0s - loss: 0.0312 - acc: 0.9916 - val_loss: 2.3122 - val_acc: 0.5522\n",
      "Epoch 23/50\n",
      "0s - loss: 0.0297 - acc: 0.9906 - val_loss: 2.2354 - val_acc: 0.5373\n",
      "Epoch 24/50\n",
      "0s - loss: 0.0248 - acc: 0.9930 - val_loss: 2.2286 - val_acc: 0.5634\n",
      "Epoch 25/50\n",
      "0s - loss: 0.0233 - acc: 0.9934 - val_loss: 2.3734 - val_acc: 0.5373\n",
      "Epoch 26/50\n",
      "0s - loss: 0.0172 - acc: 0.9972 - val_loss: 2.3186 - val_acc: 0.5541\n",
      "Epoch 27/50\n",
      "0s - loss: 0.0207 - acc: 0.9958 - val_loss: 2.4228 - val_acc: 0.5522\n",
      "Epoch 28/50\n",
      "0s - loss: 0.0241 - acc: 0.9944 - val_loss: 2.3224 - val_acc: 0.5522\n",
      "Epoch 29/50\n",
      "0s - loss: 0.0160 - acc: 0.9958 - val_loss: 2.4128 - val_acc: 0.5392\n",
      "Epoch 30/50\n",
      "0s - loss: 0.0116 - acc: 0.9967 - val_loss: 2.2848 - val_acc: 0.5616\n",
      "Epoch 31/50\n",
      "0s - loss: 0.0099 - acc: 0.9981 - val_loss: 2.4636 - val_acc: 0.5560\n",
      "Epoch 32/50\n",
      "1s - loss: 0.0078 - acc: 0.9986 - val_loss: 2.5315 - val_acc: 0.5709\n",
      "Epoch 33/50\n",
      "0s - loss: 0.0087 - acc: 0.9986 - val_loss: 2.4766 - val_acc: 0.5485\n",
      "Epoch 34/50\n",
      "0s - loss: 0.0221 - acc: 0.9934 - val_loss: 2.7372 - val_acc: 0.5429\n",
      "Epoch 35/50\n",
      "0s - loss: 0.0203 - acc: 0.9939 - val_loss: 2.4145 - val_acc: 0.5466\n",
      "Epoch 36/50\n",
      "0s - loss: 0.0121 - acc: 0.9986 - val_loss: 2.5365 - val_acc: 0.5485\n",
      "Epoch 37/50\n",
      "0s - loss: 0.0139 - acc: 0.9962 - val_loss: 2.3810 - val_acc: 0.5765\n",
      "Epoch 38/50\n",
      "1s - loss: 0.0089 - acc: 0.9986 - val_loss: 2.5223 - val_acc: 0.5541\n",
      "Epoch 39/50\n",
      "0s - loss: 0.0157 - acc: 0.9958 - val_loss: 2.6310 - val_acc: 0.5616\n",
      "Epoch 40/50\n",
      "1s - loss: 0.0083 - acc: 0.9972 - val_loss: 2.5573 - val_acc: 0.5597\n",
      "Epoch 41/50\n",
      "1s - loss: 0.0067 - acc: 0.9991 - val_loss: 2.6300 - val_acc: 0.5653\n",
      "Epoch 42/50\n",
      "1s - loss: 0.0034 - acc: 1.0000 - val_loss: 2.6698 - val_acc: 0.5672\n",
      "Epoch 43/50\n",
      "1s - loss: 0.0072 - acc: 0.9977 - val_loss: 2.6886 - val_acc: 0.5690\n",
      "Epoch 44/50\n",
      "1s - loss: 0.0131 - acc: 0.9972 - val_loss: 2.6070 - val_acc: 0.5653\n",
      "Epoch 45/50\n",
      "1s - loss: 0.0047 - acc: 0.9995 - val_loss: 2.5764 - val_acc: 0.5802\n",
      "Epoch 46/50\n",
      "1s - loss: 0.0080 - acc: 0.9977 - val_loss: 2.6428 - val_acc: 0.5634\n",
      "Epoch 47/50\n",
      "1s - loss: 0.0045 - acc: 0.9995 - val_loss: 2.7454 - val_acc: 0.5802\n",
      "Epoch 48/50\n",
      "1s - loss: 0.0059 - acc: 0.9981 - val_loss: 2.5667 - val_acc: 0.5840\n",
      "Epoch 49/50\n",
      "1s - loss: 0.0091 - acc: 0.9967 - val_loss: 2.6342 - val_acc: 0.5672\n",
      "Epoch 50/50\n",
      "1s - loss: 0.0046 - acc: 0.9995 - val_loss: 2.7544 - val_acc: 0.5914\n",
      "Network's test score [loss, accuracy]: [2.7544217768000134, 0.59141791044776115]\n"
     ]
    }
   ],
   "source": [
    "train_top_model(train_feats=train_bottleneck, train_lab=train_labels, test_feats=test_bottleneck, test_lab=test_labels,\n",
    "                model_path=modelPath, model_save=top_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Top Model on the Fully Augmented Data\n",
    "In this run we will be using the bottleneck features from taking the training data and augmenting it with the followin transformations; Vertical Flip, Horizontal Flip, 90 and 270 degree rotation, and 15, 30, and 45 degree rotation in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locations for the bottleneck and labels files that we need\n",
    "modelPath = '../model/'\n",
    "train_bottleneck = 'bottleneck_features_150fulltrans_train.npy'\n",
    "train_labels = 'labels_150fulltrans_train.npy'\n",
    "test_bottleneck = 'bottleneck_features_test.npy'\n",
    "test_labels = 'labels_test.npy'\n",
    "validation_bottleneck = 'bottleneck_features_valdation.npy'\n",
    "validation_label = 'labels_validation.npy'\n",
    "top_model_weights_path = 'top_weights_150fulltrans.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23452 samples, validate on 536 samples\n",
      "Epoch 1/50\n",
      "9s - loss: 2.1150 - acc: 0.5010 - val_loss: 0.9707 - val_acc: 0.5429\n",
      "Epoch 2/50\n",
      "9s - loss: 0.8477 - acc: 0.6011 - val_loss: 1.0201 - val_acc: 0.5690\n",
      "Epoch 3/50\n",
      "9s - loss: 0.7545 - acc: 0.6495 - val_loss: 1.0338 - val_acc: 0.5709\n",
      "Epoch 4/50\n",
      "10s - loss: 0.6726 - acc: 0.6937 - val_loss: 1.1553 - val_acc: 0.5634\n",
      "Epoch 5/50\n",
      "12s - loss: 0.6003 - acc: 0.7329 - val_loss: 1.1349 - val_acc: 0.5709\n",
      "Epoch 6/50\n",
      "13s - loss: 0.5381 - acc: 0.7633 - val_loss: 1.2890 - val_acc: 0.5784\n",
      "Epoch 7/50\n",
      "13s - loss: 0.4795 - acc: 0.7914 - val_loss: 1.3350 - val_acc: 0.5728\n",
      "Epoch 8/50\n",
      "13s - loss: 0.4376 - acc: 0.8142 - val_loss: 1.3227 - val_acc: 0.5877\n",
      "Epoch 9/50\n",
      "12s - loss: 0.3839 - acc: 0.8383 - val_loss: 1.4640 - val_acc: 0.5560\n",
      "Epoch 10/50\n",
      "12s - loss: 0.3497 - acc: 0.8528 - val_loss: 1.5536 - val_acc: 0.5765\n",
      "Epoch 11/50\n",
      "12s - loss: 0.3168 - acc: 0.8711 - val_loss: 1.6186 - val_acc: 0.5784\n",
      "Epoch 12/50\n",
      "12s - loss: 0.2859 - acc: 0.8848 - val_loss: 1.6248 - val_acc: 0.5653\n",
      "Epoch 13/50\n",
      "12s - loss: 0.2593 - acc: 0.8979 - val_loss: 1.7318 - val_acc: 0.5765\n",
      "Epoch 14/50\n",
      "12s - loss: 0.2411 - acc: 0.9047 - val_loss: 1.8024 - val_acc: 0.5746\n",
      "Epoch 15/50\n",
      "12s - loss: 0.2157 - acc: 0.9148 - val_loss: 1.8175 - val_acc: 0.6101\n",
      "Epoch 16/50\n",
      "12s - loss: 0.2096 - acc: 0.9177 - val_loss: 1.9431 - val_acc: 0.5802\n",
      "Epoch 17/50\n",
      "12s - loss: 0.1886 - acc: 0.9273 - val_loss: 2.0001 - val_acc: 0.5709\n",
      "Epoch 18/50\n",
      "12s - loss: 0.1718 - acc: 0.9351 - val_loss: 1.9693 - val_acc: 0.5765\n",
      "Epoch 19/50\n",
      "12s - loss: 0.1730 - acc: 0.9361 - val_loss: 2.0154 - val_acc: 0.5802\n",
      "Epoch 20/50\n",
      "12s - loss: 0.1525 - acc: 0.9414 - val_loss: 2.1347 - val_acc: 0.5504\n",
      "Epoch 21/50\n",
      "12s - loss: 0.1443 - acc: 0.9461 - val_loss: 2.1748 - val_acc: 0.5933\n",
      "Epoch 22/50\n",
      "12s - loss: 0.1428 - acc: 0.9473 - val_loss: 2.2186 - val_acc: 0.5896\n",
      "Epoch 23/50\n",
      "12s - loss: 0.1324 - acc: 0.9505 - val_loss: 2.2057 - val_acc: 0.5522\n",
      "Epoch 24/50\n",
      "12s - loss: 0.1307 - acc: 0.9520 - val_loss: 2.2344 - val_acc: 0.5728\n",
      "Epoch 25/50\n",
      "12s - loss: 0.1166 - acc: 0.9571 - val_loss: 2.3347 - val_acc: 0.5765\n",
      "Epoch 26/50\n",
      "12s - loss: 0.1163 - acc: 0.9586 - val_loss: 2.3327 - val_acc: 0.5728\n",
      "Epoch 27/50\n",
      "12s - loss: 0.1117 - acc: 0.9623 - val_loss: 2.3773 - val_acc: 0.5728\n",
      "Epoch 28/50\n",
      "12s - loss: 0.1153 - acc: 0.9609 - val_loss: 2.3647 - val_acc: 0.5858\n",
      "Epoch 29/50\n",
      "12s - loss: 0.1057 - acc: 0.9634 - val_loss: 2.4121 - val_acc: 0.5504\n",
      "Epoch 30/50\n",
      "12s - loss: 0.1045 - acc: 0.9645 - val_loss: 2.5405 - val_acc: 0.5690\n",
      "Epoch 31/50\n",
      "12s - loss: 0.1051 - acc: 0.9655 - val_loss: 2.4535 - val_acc: 0.5709\n",
      "Epoch 32/50\n",
      "12s - loss: 0.0940 - acc: 0.9667 - val_loss: 2.5613 - val_acc: 0.5578\n",
      "Epoch 33/50\n",
      "12s - loss: 0.0979 - acc: 0.9653 - val_loss: 2.5431 - val_acc: 0.5858\n",
      "Epoch 34/50\n",
      "12s - loss: 0.0877 - acc: 0.9693 - val_loss: 2.6461 - val_acc: 0.5765\n",
      "Epoch 35/50\n",
      "12s - loss: 0.0901 - acc: 0.9697 - val_loss: 2.7325 - val_acc: 0.5522\n",
      "Epoch 36/50\n",
      "12s - loss: 0.0879 - acc: 0.9705 - val_loss: 2.7409 - val_acc: 0.5634\n",
      "Epoch 37/50\n",
      "12s - loss: 0.0861 - acc: 0.9707 - val_loss: 2.6940 - val_acc: 0.5765\n",
      "Epoch 38/50\n",
      "12s - loss: 0.0868 - acc: 0.9715 - val_loss: 2.6151 - val_acc: 0.5672\n",
      "Epoch 39/50\n",
      "12s - loss: 0.0818 - acc: 0.9726 - val_loss: 2.5774 - val_acc: 0.5765\n",
      "Epoch 40/50\n",
      "12s - loss: 0.0830 - acc: 0.9732 - val_loss: 2.5922 - val_acc: 0.5616\n",
      "Epoch 41/50\n",
      "12s - loss: 0.0789 - acc: 0.9748 - val_loss: 2.8912 - val_acc: 0.5672\n",
      "Epoch 42/50\n",
      "12s - loss: 0.0794 - acc: 0.9736 - val_loss: 2.7895 - val_acc: 0.5821\n",
      "Epoch 43/50\n",
      "12s - loss: 0.0727 - acc: 0.9758 - val_loss: 3.0036 - val_acc: 0.5578\n",
      "Epoch 44/50\n",
      "12s - loss: 0.0768 - acc: 0.9745 - val_loss: 2.8650 - val_acc: 0.5578\n",
      "Epoch 45/50\n",
      "12s - loss: 0.0734 - acc: 0.9767 - val_loss: 2.7810 - val_acc: 0.5634\n",
      "Epoch 46/50\n",
      "12s - loss: 0.0758 - acc: 0.9768 - val_loss: 2.9774 - val_acc: 0.5485\n",
      "Epoch 47/50\n",
      "12s - loss: 0.0740 - acc: 0.9764 - val_loss: 2.8870 - val_acc: 0.5653\n",
      "Epoch 48/50\n",
      "12s - loss: 0.0725 - acc: 0.9761 - val_loss: 2.9791 - val_acc: 0.5560\n",
      "Epoch 49/50\n",
      "12s - loss: 0.0690 - acc: 0.9773 - val_loss: 2.8205 - val_acc: 0.5728\n",
      "Epoch 50/50\n",
      "12s - loss: 0.0688 - acc: 0.9786 - val_loss: 2.8448 - val_acc: 0.5728\n",
      "Network's test score [loss, accuracy]: [2.8447668943832172, 0.57276119402985071]\n"
     ]
    }
   ],
   "source": [
    "train_top_model(train_feats=train_bottleneck, train_lab=train_labels, test_feats=test_bottleneck, test_lab=test_labels,\n",
    "                model_path=modelPath, model_save=top_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Top Model at 224x224\n",
    "In this next experiment we run the model with transformations on the data at a size of 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locations for the bottleneck and labels files that we need\n",
    "modelPath = '../model/'\n",
    "train_bottleneck = 'bottleneck_features_train_224.npy'\n",
    "train_labels = 'labels_train_224.npy'\n",
    "test_bottleneck = 'bottleneck_features_test.npy'\n",
    "test_labels = 'labels_test.npy'\n",
    "validation_bottleneck = 'bottleneck_features_valdation.npy'\n",
    "validation_label = 'labels_validation.npy'\n",
    "top_model_weights_path = 'top_weights_224.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2132 samples, validate on 536 samples\n",
      "Epoch 1/50\n",
      "2s - loss: 9.0933 - acc: 0.4170 - val_loss: 8.1122 - val_acc: 0.4608\n",
      "Epoch 2/50\n",
      "2s - loss: 8.1198 - acc: 0.4634 - val_loss: 9.5227 - val_acc: 0.4030\n",
      "Epoch 3/50\n",
      "2s - loss: 7.4928 - acc: 0.5033 - val_loss: 7.4591 - val_acc: 0.4776\n",
      "Epoch 4/50\n",
      "2s - loss: 6.3570 - acc: 0.5432 - val_loss: 6.4214 - val_acc: 0.5466\n",
      "Epoch 5/50\n",
      "2s - loss: 5.6562 - acc: 0.5943 - val_loss: 5.6958 - val_acc: 0.5634\n",
      "Epoch 6/50\n",
      "2s - loss: 4.9501 - acc: 0.6196 - val_loss: 6.8575 - val_acc: 0.5168\n",
      "Epoch 7/50\n",
      "2s - loss: 4.3462 - acc: 0.6384 - val_loss: 7.0712 - val_acc: 0.4571\n",
      "Epoch 8/50\n",
      "2s - loss: 2.2618 - acc: 0.6731 - val_loss: 1.3050 - val_acc: 0.5914\n",
      "Epoch 9/50\n",
      "2s - loss: 0.7706 - acc: 0.7233 - val_loss: 1.1151 - val_acc: 0.5765\n",
      "Epoch 10/50\n",
      "2s - loss: 0.5902 - acc: 0.7617 - val_loss: 1.1186 - val_acc: 0.6101\n",
      "Epoch 11/50\n",
      "2s - loss: 0.4783 - acc: 0.8096 - val_loss: 1.2259 - val_acc: 0.6194\n",
      "Epoch 12/50\n",
      "2s - loss: 0.3681 - acc: 0.8462 - val_loss: 1.3938 - val_acc: 0.5933\n",
      "Epoch 13/50\n",
      "2s - loss: 0.3200 - acc: 0.8818 - val_loss: 1.3543 - val_acc: 0.5970\n",
      "Epoch 14/50\n",
      "2s - loss: 0.2700 - acc: 0.8987 - val_loss: 1.4228 - val_acc: 0.6213\n",
      "Epoch 15/50\n",
      "2s - loss: 0.2238 - acc: 0.9146 - val_loss: 1.4641 - val_acc: 0.5597\n",
      "Epoch 16/50\n",
      "2s - loss: 0.1715 - acc: 0.9353 - val_loss: 1.5231 - val_acc: 0.6101\n",
      "Epoch 17/50\n",
      "2s - loss: 0.1404 - acc: 0.9522 - val_loss: 1.7135 - val_acc: 0.5933\n",
      "Epoch 18/50\n",
      "2s - loss: 0.1347 - acc: 0.9461 - val_loss: 1.8070 - val_acc: 0.5970\n",
      "Epoch 19/50\n",
      "2s - loss: 0.1140 - acc: 0.9597 - val_loss: 1.6378 - val_acc: 0.6138\n",
      "Epoch 20/50\n",
      "2s - loss: 0.1024 - acc: 0.9658 - val_loss: 2.0474 - val_acc: 0.5877\n",
      "Epoch 21/50\n",
      "2s - loss: 0.0892 - acc: 0.9658 - val_loss: 2.0427 - val_acc: 0.6119\n",
      "Epoch 22/50\n",
      "2s - loss: 0.0864 - acc: 0.9719 - val_loss: 1.9937 - val_acc: 0.5653\n",
      "Epoch 23/50\n",
      "2s - loss: 0.0682 - acc: 0.9761 - val_loss: 2.4891 - val_acc: 0.5784\n",
      "Epoch 24/50\n",
      "2s - loss: 0.0682 - acc: 0.9747 - val_loss: 1.9282 - val_acc: 0.6119\n",
      "Epoch 25/50\n",
      "2s - loss: 0.0435 - acc: 0.9850 - val_loss: 2.1998 - val_acc: 0.6007\n",
      "Epoch 26/50\n",
      "2s - loss: 0.0445 - acc: 0.9850 - val_loss: 1.9286 - val_acc: 0.6250\n",
      "Epoch 27/50\n",
      "2s - loss: 0.0521 - acc: 0.9845 - val_loss: 2.3834 - val_acc: 0.5634\n",
      "Epoch 28/50\n",
      "2s - loss: 0.0627 - acc: 0.9784 - val_loss: 1.9833 - val_acc: 0.6231\n",
      "Epoch 29/50\n",
      "2s - loss: 0.0419 - acc: 0.9850 - val_loss: 2.2553 - val_acc: 0.5877\n",
      "Epoch 30/50\n",
      "2s - loss: 0.0302 - acc: 0.9902 - val_loss: 2.2751 - val_acc: 0.6138\n",
      "Epoch 31/50\n",
      "2s - loss: 0.0301 - acc: 0.9897 - val_loss: 2.2381 - val_acc: 0.6082\n",
      "Epoch 32/50\n",
      "2s - loss: 0.0190 - acc: 0.9958 - val_loss: 2.3391 - val_acc: 0.6119\n",
      "Epoch 33/50\n",
      "2s - loss: 0.0286 - acc: 0.9892 - val_loss: 2.2304 - val_acc: 0.6026\n",
      "Epoch 34/50\n",
      "2s - loss: 0.0315 - acc: 0.9883 - val_loss: 2.4235 - val_acc: 0.6119\n",
      "Epoch 35/50\n",
      "2s - loss: 0.0368 - acc: 0.9883 - val_loss: 2.3665 - val_acc: 0.6213\n",
      "Epoch 36/50\n",
      "2s - loss: 0.0132 - acc: 0.9958 - val_loss: 2.4955 - val_acc: 0.6381\n",
      "Epoch 37/50\n",
      "2s - loss: 0.0270 - acc: 0.9911 - val_loss: 2.6176 - val_acc: 0.5933\n",
      "Epoch 38/50\n",
      "2s - loss: 0.0335 - acc: 0.9864 - val_loss: 2.3849 - val_acc: 0.6250\n",
      "Epoch 39/50\n",
      "2s - loss: 0.0157 - acc: 0.9953 - val_loss: 2.4454 - val_acc: 0.6287\n",
      "Epoch 40/50\n",
      "2s - loss: 0.0160 - acc: 0.9934 - val_loss: 2.6720 - val_acc: 0.6119\n",
      "Epoch 41/50\n",
      "3s - loss: 0.0183 - acc: 0.9962 - val_loss: 2.6914 - val_acc: 0.5989\n",
      "Epoch 42/50\n",
      "3s - loss: 0.0230 - acc: 0.9930 - val_loss: 2.5981 - val_acc: 0.6082\n",
      "Epoch 43/50\n",
      "3s - loss: 0.0173 - acc: 0.9948 - val_loss: 2.8000 - val_acc: 0.5951\n",
      "Epoch 44/50\n",
      "3s - loss: 0.0182 - acc: 0.9930 - val_loss: 3.0679 - val_acc: 0.6007\n",
      "Epoch 45/50\n",
      "3s - loss: 0.0205 - acc: 0.9920 - val_loss: 2.7439 - val_acc: 0.6082\n",
      "Epoch 46/50\n",
      "3s - loss: 0.0137 - acc: 0.9953 - val_loss: 2.8064 - val_acc: 0.5896\n",
      "Epoch 47/50\n",
      "3s - loss: 0.0142 - acc: 0.9948 - val_loss: 2.5329 - val_acc: 0.5951\n",
      "Epoch 48/50\n",
      "3s - loss: 0.0188 - acc: 0.9930 - val_loss: 2.6906 - val_acc: 0.6082\n",
      "Epoch 49/50\n",
      "3s - loss: 0.0248 - acc: 0.9916 - val_loss: 2.6263 - val_acc: 0.6194\n",
      "Epoch 50/50\n",
      "4s - loss: 0.0119 - acc: 0.9944 - val_loss: 2.7021 - val_acc: 0.5989\n",
      "Network's test score [loss, accuracy]: [2.7020884663311402, 0.59888059701492535]\n"
     ]
    }
   ],
   "source": [
    "train_top_model(train_feats=train_bottleneck, train_lab=train_labels, test_feats=test_bottleneck, test_lab=test_labels,\n",
    "                model_path=modelPath, model_save=top_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "All results below are run against the train, test, validate csv files located at [Breast Cancer Github Data](https://github.com/jnarhan/Breast_Cancer/tree/master/data)\n",
    "\n",
    "### Aspect Ratio Squared Raw DDSM Images with Artifacts\n",
    "1) Run 1: 150x150 image size, 50 Epochs, Batch Size 64\n",
    "    * Network's test score [loss, accuracy]: [2.4609192387381595, 0.58582089552238803]\n",
    "2) Run 2: 150x150 image size, 50 Epochs, Batch Size 64, Full Augmentations\n",
    "    * Network's test score [loss, accuracy]: [2.8447668943832172, 0.57276119402985071]\n",
    "3) Run 3: 224x224 image size, 50 Epochs, Batch Size 64\n",
    "    * Network's test score [loss, accuracy]: [2.7020884663311402, 0.59888059701492535]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

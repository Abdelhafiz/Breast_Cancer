{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import keras.callbacks as cb\n",
    "import keras.utils.np_utils as np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers.core import Activation\n",
    "from keras import applications # For easy loading the VGG_16 Model\n",
    "from skimage import color\n",
    "# Image loading and other helper functions\n",
    "import dwdii_bc_model_helper as bc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_Prep\n",
    "The ollowing function takes the 8bit grayscale images that we are using and converts them to 8bit rgb while at the same time changing the pixles to a scale of 0 to 255. These image parameters are required by the VGG_16 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def VGG_Prep(img_data):\n",
    "    \"\"\"\n",
    "    :param img_data: training or test images of shape [#images, height, width]\n",
    "    :return: the array transformed to the correct shape for the VGG network\n",
    "                shape = [#images, height, width, 3] transforms to rgb and reshapes\n",
    "    \"\"\"\n",
    "    images = np.zeros([len(img_data), img_data.shape[1], img_data.shape[2], 3])\n",
    "    for i in range(0, len(img_data)):\n",
    "        im = 255 - (img_data[i] * 255)  # Orginal imagnet images were not rescaled\n",
    "        im = color.gray2rgb(im)\n",
    "        images[i] = im\n",
    "    return(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG_16 Bottleneck\n",
    "The following function leverages Daniel's image loader function and performs the following:\n",
    "1. Loads in the images using the train, test, and validation csv files.\n",
    "2. Prepares the images using the VGG_Prep function\n",
    "3. Loads the VGG_16 model with the cassification layers removed.\n",
    "4. Runs each of the images for the training, test, and validation sets (if included) through the model.\n",
    "5. Saves out .npy files containing the bottleneck features from the VGG_16 model predictions and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg16_bottleneck(trainPath, testPath, imagePath, modelPath, size, balance = True, verbose = True, verboseFreq = 50, valPath = 'None'):\n",
    "    # Loading data\n",
    "    metaTr, metaTr2, mCountsTr = bc.load_training_metadata(trainPath, balance, verbose)\n",
    "    lenTrain = len(metaTr)\n",
    "    X_train, Y_train = bc.load_data(trainPath, imagePath, maxData = lenTrain, verboseFreq = verboseFreq, imgResize=size)\n",
    "    \n",
    "    metaTest, meataT2, mCountsT = bc.load_training_metadata(testPath, balance, verbose)\n",
    "    lenTest = len(metaTest)\n",
    "    X_test, Y_test = bc.load_data(testPath, imagePath, maxData = lenTrain, verboseFreq = verboseFreq, imgResize=size)\n",
    "    \n",
    "    X_train = VGG_Prep(X_train)\n",
    "    X_test = VGG_Prep(X_test)\n",
    "        \n",
    "    print('Loading the VGG_16 Model')\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "        \n",
    "    # Generating the bottleneck features for the training data\n",
    "    print('Evaluating the VGG_16 Model on the Training Data')\n",
    "    bottleneck_features_train = model.predict(X_train)\n",
    "    \n",
    "    # Saving the bottleneck features for the training data\n",
    "    featuresTrain = os.path.join(modelPath, 'bottleneck_features_train.npy')\n",
    "    labelsTrain = os.path.join(modelPath, 'labels_train.npy')\n",
    "    np.save(open(featuresTrain, 'wb'), bottleneck_features_train)\n",
    "    np.save(open(labelsTrain, 'wb'), Y_train)\n",
    "\n",
    "    # Generating the bottleneck features for the test data\n",
    "    print('Evaluating the VGG_16 Model on the Test Data')\n",
    "    bottleneck_features_test = model.predict(X_test)\n",
    "    \n",
    "    # Saving the bottleneck features for the test data\n",
    "    featuresTest = os.path.join(modelPath, 'bottleneck_features_test.npy')\n",
    "    labelsTest = os.path.join(modelPath, 'labels_test.npy')\n",
    "    np.save(open(featuresTest, 'wb'), bottleneck_features_test)\n",
    "    np.save(open(labelsTest, 'wb'), Y_test)\n",
    "    \n",
    "    if valPath != 'None':\n",
    "        metaVal, metaV2, mCountsV = bc.load_training_metadata(valPath, verbose = verbose, balanceViaRemoval = False)\n",
    "        lenVal = len(metaVal)\n",
    "        X_val, Y_val = bc.load_data(valPath, imagePath, maxData = lenVal, verboseFreq = verboseFreq, imgResize=size)\n",
    "        X_val = VGG_Prep(X_val)\n",
    "        \n",
    "        # Generating the bottleneck features for the test data\n",
    "        print('Evaluating the VGG_16 Model on the Validataion Data')\n",
    "        bottleneck_features_val = model.predict(X_val)\n",
    "    \n",
    "        # Saving the bottleneck features for the test data\n",
    "        featuresVal = os.path.join(modelPath, 'bottleneck_features_validation.npy')\n",
    "        labelsVal = os.path.join(modelPath, 'labels_validation.npy')\n",
    "        np.save(open(featuresVal, 'wb'), bottleneck_features_val)\n",
    "        np.save(open(labelsVal, 'wb'), Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model on the Train, Test, and Validation Data\n",
    "1) The first test is on the rescaled and squared off images maintaining aspect ratio without the artifacts removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global variables for loading the data\n",
    "imagePath = '../images/ddsm/png/'\n",
    "trainDataPath = '../images/ddsm/ddsm_train.csv'\n",
    "testDataPath = '../images/ddsm/ddsm_test.csv'\n",
    "valDataPath = '../images/ddsm/ddsm_val.csv'\n",
    "imgResize = (150, 150) # can go up to (224, 224)\n",
    "modelPath = '../model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Balance\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 2685\n",
      "balanaceViaRemoval.avgE: 1318\n",
      "balanaceViaRemoval.theshold: 1318.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 862\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 2685\n",
      "balanaceViaRemoval.avgE: 1318\n",
      "balanaceViaRemoval.theshold: 1318.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 531\n",
      "malignant 739\n",
      "normal 862\n",
      "0.0000: A_0152_1.RIGHT_MLO.LJPEG.png\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dwdii_bc_model_helper.py:161: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  X_data = np.zeros([total, x, y])\n",
      "dwdii_bc_model_helper.py:162: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  Y_data = np.zeros([total, 1], dtype=np.int8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0235: C_0091_1.LEFT_CC.LJPEG.png\n",
      "0.0469: A_0707_1.LEFT_CC.LJPEG.png\n",
      "0.0704: A_0534_1.RIGHT_CC.LJPEG.png\n",
      "0.0938: A_1055_1.LEFT_CC.LJPEG.png\n",
      "0.1173: C_0415_1.RIGHT_MLO.LJPEG.png\n",
      "0.1407: C_0305_1.LEFT_CC.LJPEG.png\n",
      "0.1642: A_1061_1.LEFT_CC.LJPEG.png\n",
      "0.1876: C_0383_1.RIGHT_CC.LJPEG.png\n",
      "0.2111: B_3120_1.RIGHT_CC.LJPEG.png\n",
      "0.2345: C_0321_1.LEFT_MLO.LJPEG.png\n",
      "0.2580: B_3412_1.LEFT_CC.LJPEG.png\n",
      "0.2814: A_0139_1.RIGHT_CC.LJPEG.png\n",
      "0.3049: B_3098_1.RIGHT_MLO.LJPEG.png\n",
      "0.3283: A_0056_1.LEFT_MLO.LJPEG.png\n",
      "0.3518: C_0396_1.LEFT_CC.LJPEG.png\n",
      "0.3752: C_0337_1.RIGHT_CC.LJPEG.png\n",
      "0.3987: A_0572_1.RIGHT_MLO.LJPEG.png\n",
      "0.4221: A_1057_1.LEFT_CC.LJPEG.png\n",
      "0.4456: B_3662_1.LEFT_MLO.LJPEG.png\n",
      "0.4690: C_0032_1.RIGHT_CC.LJPEG.png\n",
      "0.4925: A_1056_1.RIGHT_CC.LJPEG.png\n",
      "0.5159: B_3458_1.RIGHT_MLO.LJPEG.png\n",
      "0.5394: A_0067_1.RIGHT_CC.LJPEG.png\n",
      "0.5629: B_3469_1.RIGHT_CC.LJPEG.png\n",
      "0.5863: A_0227_1.LEFT_CC.LJPEG.png\n",
      "0.6098: C_0280_1.RIGHT_MLO.LJPEG.png\n",
      "0.6332: A_0417_1.LEFT_CC.LJPEG.png\n",
      "0.6567: C_0130_1.LEFT_MLO.LJPEG.png\n",
      "0.6801: C_0414_1.RIGHT_MLO.LJPEG.png\n",
      "0.7036: C_0408_1.LEFT_CC.LJPEG.png\n",
      "0.7270: C_0192_1.LEFT_CC.LJPEG.png\n",
      "0.7505: C_0148_1.LEFT_CC.LJPEG.png\n",
      "0.7739: C_0161_1.LEFT_MLO.LJPEG.png\n",
      "0.7974: C_0384_1.RIGHT_MLO.LJPEG.png\n",
      "0.8208: C_0285_1.RIGHT_MLO.LJPEG.png\n",
      "0.8443: A_0604_1.LEFT_MLO.LJPEG.png\n",
      "0.8677: C_0017_1.RIGHT_CC.LJPEG.png\n",
      "0.8912: B_3118_1.LEFT_CC.LJPEG.png\n",
      "0.9146: B_3367_1.LEFT_MLO.LJPEG.png\n",
      "0.9381: A_0217_1.LEFT_CC.LJPEG.png\n",
      "0.9615: A_0009_1.LEFT_MLO.LJPEG.png\n",
      "0.9850: B_3432_1.RIGHT_MLO.LJPEG.png\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 658\n",
      "balanaceViaRemoval.avgE: 326\n",
      "balanaceViaRemoval.theshold: 326.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 215\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 658\n",
      "balanaceViaRemoval.avgE: 326\n",
      "balanaceViaRemoval.theshold: 326.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 142\n",
      "malignant 179\n",
      "normal 215\n",
      "0.0000: A_1105_1.RIGHT_CC.LJPEG.png\n",
      "0.0235: C_0219_1.LEFT_MLO.LJPEG.png\n",
      "0.0469: C_0272_1.LEFT_MLO.LJPEG.png\n",
      "0.0704: A_0238_1.LEFT_MLO.LJPEG.png\n",
      "0.0938: C_0272_1.LEFT_CC.LJPEG.png\n",
      "0.1173: C_0395_1.LEFT_MLO.LJPEG.png\n",
      "0.1407: A_0527_1.LEFT_MLO.LJPEG.png\n",
      "0.1642: C_0473_1.RIGHT_MLO.LJPEG.png\n",
      "0.1876: B_3387_1.RIGHT_CC.LJPEG.png\n",
      "0.2111: C_0132_1.LEFT_CC.LJPEG.png\n",
      "0.2345: A_0112_1.RIGHT_CC.LJPEG.png\n",
      "Loading the VGG_16 Model\n",
      "Evaluating the VGG_16 Model on the Training Data\n",
      "Evaluating the VGG_16 Model on the Test Data\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 18\n",
      "malignant 34\n",
      "normal 142\n",
      "Raw Balance\n",
      "----------------\n",
      "benign 18\n",
      "malignant 34\n",
      "normal 142\n",
      "balanaceViaRemoval.avgE: 64\n",
      "balanaceViaRemoval.theshold: 64.0\n",
      "\n",
      "After Balancing\n",
      "----------------\n",
      "benign 18\n",
      "malignant 34\n",
      "normal 38\n",
      "0.0000: C_0062_1.LEFT_CC.LJPEG.png\n",
      "0.2577: C_0320_1.RIGHT_MLO.LJPEG.png\n",
      "Evaluating the VGG_16 Model on the Validataion Data\n"
     ]
    }
   ],
   "source": [
    "vgg16_bottleneck(trainDataPath, testDataPath, imagePath, modelPath, imgResize, \n",
    "                 balance = True, verbose = True, verboseFreq = 50, valPath = valDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Top Model\n",
    "This function takes the bottleneck features from the bottleneck function and applies a shallow CNN to these features to classify the images. The function needs to be pointed at the locations of the training and test features along with the training and test labels. You can use the epoch and batch size variables to control the number of images to show to the model and the number of training epochs. The model save variabler alows for saving of the final model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_model(train_feats, train_lab, test_feats, test_lab, model_path, model_save, epoch = 50, batch = 64):\n",
    "    train_bottleneck = os.path.join(model_path, train_feats)\n",
    "    train_labels = os.path.join(model_path, train_lab)\n",
    "    test_bottleneck = os.path.join(model_path, test_feats)\n",
    "    test_labels = os.path.join(model_path, test_lab)\n",
    "    \n",
    "    history = LossHistory()\n",
    "    \n",
    "    X_train = np.load(train_bottleneck)\n",
    "    Y_train = np.load(train_labels)\n",
    "    Y_train = np_utils.to_categorical(Y_train, nb_classes=3)\n",
    "    \n",
    "    X_test = np.load(test_bottleneck)\n",
    "    Y_test = np.load(test_labels)\n",
    "    Y_test = np_utils.to_categorical(Y_test, nb_classes=3)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=X_train.shape[1:]))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "    # try Adadelta and Adam\n",
    "    model.compile(optimizer='adadelta',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              nb_epoch=epoch,\n",
    "              batch_size=batch,\n",
    "              callbacks=[history],\n",
    "              validation_data=(X_test, Y_test),\n",
    "              verbose=2)\n",
    "    \n",
    "    score = model.evaluate(X_test, Y_test, batch_size=16, verbose=0)\n",
    "\n",
    "    print \"Network's test score [loss, accuracy]: {0}\".format(score)\n",
    "    \n",
    "    model.save_weights(model_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Top Model\n",
    "The following runs the top model classifier on the bottleneck features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Locations for the bottleneck and labels files that we need\n",
    "modelPath = '../model/'\n",
    "train_bottleneck = 'bottleneck_features_train.npy'\n",
    "train_labels = 'labels_train.npy'\n",
    "test_bottleneck = 'bottleneck_features_test.npy'\n",
    "test_labels = 'labels_test.npy'\n",
    "validation_bottleneck = 'bottleneck_features_valdation.npy'\n",
    "validation_label = 'labels_validation.npy'\n",
    "top_model_weights_path = 'top_weights02.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2132 samples, validate on 536 samples\n",
      "Epoch 1/50\n",
      "0s - loss: 6.5851 - acc: 0.4508 - val_loss: 5.6644 - val_acc: 0.4571\n",
      "Epoch 2/50\n",
      "0s - loss: 3.4335 - acc: 0.5563 - val_loss: 2.2755 - val_acc: 0.5000\n",
      "Epoch 3/50\n",
      "0s - loss: 1.0971 - acc: 0.6393 - val_loss: 1.2428 - val_acc: 0.5243\n",
      "Epoch 4/50\n",
      "0s - loss: 0.6560 - acc: 0.7317 - val_loss: 1.3331 - val_acc: 0.4963\n",
      "Epoch 5/50\n",
      "0s - loss: 0.5417 - acc: 0.7767 - val_loss: 1.3829 - val_acc: 0.5019\n",
      "Epoch 6/50\n",
      "0s - loss: 0.4130 - acc: 0.8354 - val_loss: 1.4304 - val_acc: 0.5392\n",
      "Epoch 7/50\n",
      "0s - loss: 0.3163 - acc: 0.8752 - val_loss: 1.5420 - val_acc: 0.5187\n",
      "Epoch 8/50\n",
      "0s - loss: 0.2728 - acc: 0.8963 - val_loss: 1.5069 - val_acc: 0.5168\n",
      "Epoch 9/50\n",
      "0s - loss: 0.2050 - acc: 0.9282 - val_loss: 1.6816 - val_acc: 0.5243\n",
      "Epoch 10/50\n",
      "0s - loss: 0.1788 - acc: 0.9273 - val_loss: 1.6940 - val_acc: 0.5485\n",
      "Epoch 11/50\n",
      "0s - loss: 0.1626 - acc: 0.9465 - val_loss: 1.7204 - val_acc: 0.5299\n",
      "Epoch 12/50\n",
      "0s - loss: 0.1236 - acc: 0.9578 - val_loss: 1.7611 - val_acc: 0.5243\n",
      "Epoch 13/50\n",
      "0s - loss: 0.1066 - acc: 0.9662 - val_loss: 1.8645 - val_acc: 0.5112\n",
      "Epoch 14/50\n",
      "0s - loss: 0.0929 - acc: 0.9662 - val_loss: 1.8626 - val_acc: 0.5280\n",
      "Epoch 15/50\n",
      "0s - loss: 0.0764 - acc: 0.9784 - val_loss: 1.8900 - val_acc: 0.5373\n",
      "Epoch 16/50\n",
      "1s - loss: 0.0669 - acc: 0.9761 - val_loss: 1.8557 - val_acc: 0.5317\n",
      "Epoch 17/50\n",
      "0s - loss: 0.0542 - acc: 0.9878 - val_loss: 1.9861 - val_acc: 0.5448\n",
      "Epoch 18/50\n",
      "0s - loss: 0.0424 - acc: 0.9892 - val_loss: 2.0198 - val_acc: 0.5578\n",
      "Epoch 19/50\n",
      "0s - loss: 0.0433 - acc: 0.9883 - val_loss: 2.0364 - val_acc: 0.5448\n",
      "Epoch 20/50\n",
      "0s - loss: 0.0405 - acc: 0.9892 - val_loss: 2.5172 - val_acc: 0.5131\n",
      "Epoch 21/50\n",
      "0s - loss: 0.0325 - acc: 0.9911 - val_loss: 2.1068 - val_acc: 0.5578\n",
      "Epoch 22/50\n",
      "0s - loss: 0.0312 - acc: 0.9916 - val_loss: 2.3122 - val_acc: 0.5522\n",
      "Epoch 23/50\n",
      "0s - loss: 0.0297 - acc: 0.9906 - val_loss: 2.2354 - val_acc: 0.5373\n",
      "Epoch 24/50\n",
      "0s - loss: 0.0248 - acc: 0.9930 - val_loss: 2.2286 - val_acc: 0.5634\n",
      "Epoch 25/50\n",
      "0s - loss: 0.0233 - acc: 0.9934 - val_loss: 2.3734 - val_acc: 0.5373\n",
      "Epoch 26/50\n",
      "0s - loss: 0.0172 - acc: 0.9972 - val_loss: 2.3186 - val_acc: 0.5541\n",
      "Epoch 27/50\n",
      "0s - loss: 0.0207 - acc: 0.9958 - val_loss: 2.4228 - val_acc: 0.5522\n",
      "Epoch 28/50\n",
      "0s - loss: 0.0241 - acc: 0.9944 - val_loss: 2.3224 - val_acc: 0.5522\n",
      "Epoch 29/50\n",
      "0s - loss: 0.0160 - acc: 0.9958 - val_loss: 2.4128 - val_acc: 0.5392\n",
      "Epoch 30/50\n",
      "0s - loss: 0.0116 - acc: 0.9967 - val_loss: 2.2848 - val_acc: 0.5616\n",
      "Epoch 31/50\n",
      "0s - loss: 0.0099 - acc: 0.9981 - val_loss: 2.4636 - val_acc: 0.5560\n",
      "Epoch 32/50\n",
      "1s - loss: 0.0078 - acc: 0.9986 - val_loss: 2.5315 - val_acc: 0.5709\n",
      "Epoch 33/50\n",
      "0s - loss: 0.0087 - acc: 0.9986 - val_loss: 2.4766 - val_acc: 0.5485\n",
      "Epoch 34/50\n",
      "0s - loss: 0.0221 - acc: 0.9934 - val_loss: 2.7372 - val_acc: 0.5429\n",
      "Epoch 35/50\n",
      "0s - loss: 0.0203 - acc: 0.9939 - val_loss: 2.4145 - val_acc: 0.5466\n",
      "Epoch 36/50\n",
      "0s - loss: 0.0121 - acc: 0.9986 - val_loss: 2.5365 - val_acc: 0.5485\n",
      "Epoch 37/50\n",
      "0s - loss: 0.0139 - acc: 0.9962 - val_loss: 2.3810 - val_acc: 0.5765\n",
      "Epoch 38/50\n",
      "1s - loss: 0.0089 - acc: 0.9986 - val_loss: 2.5223 - val_acc: 0.5541\n",
      "Epoch 39/50\n",
      "0s - loss: 0.0157 - acc: 0.9958 - val_loss: 2.6310 - val_acc: 0.5616\n",
      "Epoch 40/50\n",
      "1s - loss: 0.0083 - acc: 0.9972 - val_loss: 2.5573 - val_acc: 0.5597\n",
      "Epoch 41/50\n",
      "1s - loss: 0.0067 - acc: 0.9991 - val_loss: 2.6300 - val_acc: 0.5653\n",
      "Epoch 42/50\n",
      "1s - loss: 0.0034 - acc: 1.0000 - val_loss: 2.6698 - val_acc: 0.5672\n",
      "Epoch 43/50\n",
      "1s - loss: 0.0072 - acc: 0.9977 - val_loss: 2.6886 - val_acc: 0.5690\n",
      "Epoch 44/50\n",
      "1s - loss: 0.0131 - acc: 0.9972 - val_loss: 2.6070 - val_acc: 0.5653\n",
      "Epoch 45/50\n",
      "1s - loss: 0.0047 - acc: 0.9995 - val_loss: 2.5764 - val_acc: 0.5802\n",
      "Epoch 46/50\n",
      "1s - loss: 0.0080 - acc: 0.9977 - val_loss: 2.6428 - val_acc: 0.5634\n",
      "Epoch 47/50\n",
      "1s - loss: 0.0045 - acc: 0.9995 - val_loss: 2.7454 - val_acc: 0.5802\n",
      "Epoch 48/50\n",
      "1s - loss: 0.0059 - acc: 0.9981 - val_loss: 2.5667 - val_acc: 0.5840\n",
      "Epoch 49/50\n",
      "1s - loss: 0.0091 - acc: 0.9967 - val_loss: 2.6342 - val_acc: 0.5672\n",
      "Epoch 50/50\n",
      "1s - loss: 0.0046 - acc: 0.9995 - val_loss: 2.7544 - val_acc: 0.5914\n",
      "Network's test score [loss, accuracy]: [2.7544217768000134, 0.59141791044776115]\n"
     ]
    }
   ],
   "source": [
    "train_top_model(train_feats=train_bottleneck, train_lab=train_labels, test_feats=test_bottleneck, test_lab=test_labels,\n",
    "                model_path=modelPath, model_save=top_model_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "All results below are run against the train, test, validate csv files located at [Breast Cancer Github Data](https://github.com/jnarhan/Breast_Cancer/tree/master/data)\n",
    "\n",
    "### Aspect Ratio Squared Raw DDSM Images with Artifacts\n",
    "1) Run 1: 150x150 image size 50 Epochs, Batch Size 64\n",
    "    * Network's test score [loss, accuracy]: [2.4609192387381595, 0.58582089552238803]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
